**CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision**
![[clap.png]]
## 一、背景介绍

现有的方法通常需要大量的训练数据，并且在面对新的任务或数据集时，往往需要重新训练模型。此外，现有的二进制代码表示学习方法在少样本（fewshot）和零样本（zeroshot）学习场景下表现不佳。这些场景下，模型需要在几乎没有或完全没有训练样本的情况下进行学习和推理。

作者借鉴CLIP的思想，提出了模型，通过大规模的数据生成器自动生成二进制代码及其对应的自然语言解释，从而构建一个大规模的训练数据集。通过对比学习，CLAP能够将二进制代码和自然语言解释对齐，生成更具语义信息的二进制代码嵌入

## 二、系统设计

### 1、数据生成器

数据生成器的主要任务是自动生成大规模的二进制代码和对应的自然语言解释对。这一部分的设计包括以下几个关键步骤：

#### 1.1 二进制代码的生成

利用Ubuntu仓库中的C/C++包作为源代码基础。这些包通过Ubuntu的包管理器进行自动化编译和源代码获取。

同时使用多种编译器（如GCC{7,9,11}和Clang{9,11,12}）和优化级别（O{03}和Os）来编译源代码，：为了确保生成的二进制代码质量，排除源代码段少于三行或二进制代码中基本块少于三个的情况。

#### 1.2 自然语言解释的生成

使用GPT3.5生成源代码的自然语言解释。这些解释通过以下步骤生成：

1、对源代码的功能进行简洁明了的总结。

2、不包含源代码中因编译而丢失的细节（如变量名）。

3、为解释添加与代码功能相关的标签。

#### 1.3 数据对齐

二进制代码与自然语言解释的对齐：通过源代码作为中介，将自然语言解释与对应的二进制代码对齐。这种对齐方式使得每个二进制代码片段都拥有一个对应的自然语言解释，从而形成大规模的对齐数据对。

![[clap-align.jpg]]

### 2. 对比语言汇编预训练引擎

#### 2.1 模型架构（CLAPASM）

CLAPASM基于RoBERTa基础架构，包含110M参数。模型通过平均最后一个Transformer层的输出作为汇编代码的嵌入表示。

首先通过引入指令嵌入来区分汇编指令的边界，使模型能够有效处理单个指令。

然后保留汇编代码中的相对地址关系，处理跳转指令，从而保留控制流信息。

最后使用WordPiece算法对汇编代码进行分词，保留如函数调用参数和外部函数名等关键信息。

跳转关系：通过共享跳转符号和目标指令的嵌入参数，增强模型对跳转关系的理解。

#### 2.2 汇编编码器预训练

这里参考了Bert模型，通过选择性地遮蔽汇编上下文中的令牌，进行MLM和 JTP任务的预训练。

损失函数：预训练阶段的损失函数是MLM和JTP任务的组合，通过优化损失函数来训练模型。

#### 2.3 对比预训练

对比学习：利用对比学习方法，将汇编代码编码器与预训练的文本编码器对齐。通过InfoNCE损失函数，最大化正样本对的相似度，最小化负样本对的相似度。

![[clap-le.jpg]] 

这部分也是我思考模型参照的部分，将汇编代码的编码器与源码编码器训练对齐。

### 3、零样本推理能力

在多分类任务中，将汇编代码编码为嵌入，同时生成多个提示的文本嵌入，通过计算嵌入之间的相似度来确定分类结果。

## 三、实验评估

在实验评估方面，主要提出了三个任务，

1、评估模型在识别两个二进制代码片段相似度方面的性能。

2、评估模型在识别与加密算法相关的函数方面的性能。

 3、评估模型在对不同网络协议相关的函数进行分类方面的性能。

采用了线性探测和零样本（少样本）学习两类方法进行学习比较。

### 1、评估指标

l 召回率在BCSD任务中，评估模型在10,000个函数中找到正确匹配函数的能力。

l MRR评估模型在排名中的平均倒数排名，用于衡量模型的排序性能。

l 在分类任务中，评估模型的分类准确率。

### 2、实验结果

 BCSD任务：CLAP在Recall@1和MRR指标上均优于其他基线模型，特别是零样本模型。例如，CLAP在Recall@1指标上达到了83.3%，而当前领先的监督模型仅达到了57.1%。

 加密函数识别：

即使在只有1%训练数据的情况下，CLAP的性能也超过了其他模型在完整训练集上的性能。同样于协议分类。

就未来的研究前景，作者总结出可以探索将自然语言与源代码更深入地结合，以进一步提高模型的性能。然后可以选择多样化数据源，包括不同的编译选项或平台，以及使用不同的语言模型生成源代码解释。同时可以结合更广泛的语义分析任务，例如使用自然语言进行二进制代码搜索和函数签名恢复等。