不管是CLIP的对比学习还是Bert中的掩码部分，都是借鉴了VLM学习的一部分，下面对阅读的VLM进行一个总结，这种方法将llm扩展到视觉语言模型，更有借鉴意义。在阅读之前提出两个小问题：
1. 这个模型是用了几个模型，一整个模型还是很多小模型
2. 这个模型输入输出是什么，怎么处理的
### 一、引言
随着大型语言模型（LLM）的流行，将这些模型扩展到视觉领域成为一个重要方向。视觉语言模型（VLM）在许多应用中具有巨大潜力，如视觉助手和基于文本描述生成图像的生成模型。然而，将语言与视觉连接起来并非易事，因为语言是离散的，而视觉是在更高维度的空间中演变的，且概念并不总是可以轻松离散化的。这篇论文旨在为VLM提供一个入门介绍，帮助希望进入该领域的研究人员和学生。

### 二、VLM的家族分类

- **基于对比学习的VLM**：CLIP是一个典型的例子，它通过区分正负样本对，学习将图像和文本映射到同一嵌入空间。CLIP的创新之处在于训练一个随机初始化的视觉和文本编码器，使其将图像和对应标题的表示映射到相似的嵌入向量。CLIP在零样本分类任务中表现出色，并在多个鲁棒性基准测试中超越了监督学习模型。
- **基于掩码的VLM**：FLAVA和MaskVLM是两个例子。FLAVA的架构包括图像编码器、文本编码器和多模态编码器，通过掩码图像和文本并重建它们来进行训练。MaskVLM则直接在像素空间和文本令牌空间中应用掩码，利用不同模态之间的信息流来训练模型。掩码策略的优点是可以从信息论的角度来理解VLM的目标，将掩码和生成式模型的目标统一起来。
- **生成式VLM**：CoCa、Chameleon和CM3leon是生成式VLM的例子，其实属于一个交互的状态。CoCa学习了一个完整的文本编码器和解码器，能够进行多模态理解任务。Chameleon和CM3leon是多模态生成模型，能够生成文本和图像。生成式模型的一个优势是它们可以用于判别任务，如分类或字幕预测，而无需重新训练。
- **基于预训练骨干网络的VLM**：Frozen和MiniGPT是两个例子。Frozen通过一个轻量级的映射网络将视觉特征投影到文本令牌嵌入空间，而保持语言模型冻结。MiniGPT则使用一个简单的线性投影层来对齐图像表示和大型语言模型（LLM）的输入空间。这两种模型都展示了利用预训练骨干网络来学习视觉和语言之间的映射的可行性。

### 三、VLM的训练方法

- **训练数据**：训练数据的质量和多样性是提高VLM性能的关键因素。DataComp基准测试提出了一种评估图像-文本数据集的方法，关注于设计能够在38个下游任务中实现强大零样本和检索性能的图像-文本数据集。数据修剪方法可以分为基于启发式的方法、基于预训练VLM的方法和旨在创建多样化和平衡数据集的方法。
- **数据增强**：数据增强是提高模型性能的一种有效方法。SLIP通过引入辅助的自监督损失项来增强视觉编码器的鲁棒性。CLIP-rocket则提出了将自监督损失项扩展到跨模态的方法，通过不对称地增强图像-文本对来提高模型的性能。
- **训练软件和计算资源**：训练VLM需要大量的计算资源。CLIP和OpenCLIP等模型使用了大量的GPU资源进行训练。然而，通过采用正确的训练策略，如使用高质量的数据集和掩码策略，可以降低训练成本。此外，torch.compile和xformers等软件的发展也显著提高了训练效率。
- **模型选择**：在选择模型时，需要考虑研究目标和可用资源。对比学习模型如CLIP适合于关联文本与视觉概念，而掩码策略模型适合于需要重建数据的任务。生成式模型在图像生成方面表现出色，但计算成本较高。利用预训练的骨干网络可以降低计算成本，但可能受到预训练模型的局限。

### 四、VLM的评估方法

- **视觉语言能力评估**：图像描述、文本-图像一致性和视觉问答是常见的评估VLM视觉语言能力的方法。COCO描述数据集和挑战赛通过与参考描述的比较来评估生成描述的质量。CLIPScore利用CLIP模型来预测描述与图像的接近程度，而VQAScore则通过将文本提示直接传递给视觉问答模型来评估文本到图像的一致性。
- **偏见与差异评估**：VLM在分类任务中可能表现出偏见，如对人物属性的偏见。可以通过对分类结果进行分析来评估偏见。此外，还可以通过嵌入空间分析来揭示VLM中隐含的关系。例如，Grounded-WEAT和Grounded-SEAT测试可以测量VLM中与人类隐含关联相似的偏见。
- **幻觉与记忆评估**：VLM可能会产生幻觉或记住训练数据中的特定信息。CHAIR基准测试用于评估对象幻觉，而POPE则通过二元投票问题来评估对象幻觉。此外，还可以利用大型语言模型如GPT-4来评估VLM的指令遵循能力、对象幻觉定位能力和对问题的回答能力。

### 五、VLM的扩展应用

- **视频理解**：将VLM扩展到视频数据可以提高模型对动态场景的理解能力。视频BERT是第一个成功的视频语言模型，它通过将视频和文本令牌融合在一起并使用VideoBERT的掩码语言建模目标进行训练。VideoOFA提出了一种两阶段预训练框架，将单个生成式图像-文本VLM适应于视频-文本任务。Video-LLaMA则通过将视频编码器与现有的大型语言模型对齐来增强视频理解能力。

### 六、结论与未来方向

尽管VLM在图像-语言映射方面取得了进展，但仍存在许多未解决的问题。例如，模型的可靠性、数据效率、幻觉和偏见等问题仍然是研究的挑战。未来的研究需要在这些方面取得突破，以推动VLM的更广泛应用。此外，视频理解是一个新的研究方向，需要克服数据稀缺、计算成本高和评估方法不足等挑战。

就阅读前提出的小问题进行一个回答：
##### 一、模型构成

视觉语言模型（VLM）可以是一个整体模型，也可以由多个小模型组成。具体取决于模型的设计和应用场景。以下是一些常见的VLM架构类型进行的分类：

1. **单一整体模型**：单一整体模型是指将视觉和语言处理集成在一个统一的模型架构中。这种模型通常通过多模态表示学习，将图像和文本映射到一个共享的特征空间。
   - Eg.
     - **CLIP**：由OpenAI开发的CLIP模型是一个典型的单一整体模型。它包含一个视觉编码器（如ResNet或Vision Transformer）和一个文本编码器（如Transformer）。这两个编码器将图像和文本分别转换为嵌入向量，然后通过对比学习的方式训练模型，使匹配的图像-文本对在特征空间中更接近。
     - **FLAVA**：Facebook AI Research（FAIR）开发的FLAVA模型也是一个单一整体模型。它包括图像编码器、文本编码器和多模态编码器。图像编码器使用Vision Transformer（ViT）处理图像，文本编码器使用Transformer处理文本，多模态编码器则融合图像和文本的特征。FLAVA通过掩码图像和文本并重建它们来进行训练。

2. **多模型组合**：多模型组合是指将多个独立的小模型组合在一起，每个小模型负责处理特定的任务或模态。这种架构通常用于需要处理复杂多模态任务的场景。
   - Eg.
     - **CoCa**：CoCa模型结合了对比学习和生成式模型的特点。它包含一个图像编码器和一个文本解码器，能够生成图像字幕。此外，CoCa还可以通过多任务学习的方式，同时处理图像分类、字幕生成等多种任务。
     - **Chameleon和CM3leon**：这些模型是多模态生成模型，能够生成文本和图像。它们通常由多个组件组成，如图像生成模块、文本生成模块和跨模态融合模块。
     - **MiniGPT**：MiniGPT系列模型通过将视觉编码器与预训练的大型语言模型（LLM）结合，利用线性投影层对齐图像和文本的表示。这种架构利用了预训练的LLM（如Vicuna）和视觉编码器（如BLIP-2中的Q-Former和ViT骨干网络），通过训练线性投影层来实现视觉和语言的融合。

#### 二、输入输出及处理方式

1. **输入处理**：
   - **图像输入**：图像通常被预处理为固定大小的张量，然后通过视觉编码器（如CNN或ViT）转换为特征向量。这些特征向量可以是全局特征（如图像的整体表示）或局部特征（如图像的局部区域表示）。例如，在CLIP中，图像被分割为多个patch，每个patch通过ViT转换为嵌入向量。
   - **文本输入**：文本通常被分词后转换为词嵌入向量。这些嵌入向量通过文本编码器（如Transformer）转换为文本特征向量。在FLAVA中，文本被分词后通过Transformer编码器生成文本特征。
   - **多模态融合**：对于需要处理多模态输入的模型（如FLAVA和MiniGPT），图像和文本的特征向量会被进一步融合。例如，FLAVA中的多模态编码器通过交叉注意力机制将图像和文本特征融合在一起。

2. **输出处理**：
   - **图像输出**：生成式VLM（如DALL-E、Stable Diffusion）可以生成图像。这些模型通常将文本提示转换为图像特征，然后通过生成网络（如扩散模型或自回归模型）生成图像。例如，在Stable Diffusion中，文本提示被编码为条件向量，然后通过U-Net结构的扩散模型生成图像。
   - **文本输出**：VLM可以生成文本描述（如字幕生成）。例如，CLIP可以通过对比学习的方式，将图像嵌入与文本嵌入进行匹配，从而生成与图像相关的文本描述。CoCa模型的文本解码器可以直接生成图像的字幕。
   - **其他输出**：VLM还可以用于其他任务，如视觉问答（VQA）。在这种情况下，模型的输出是针对输入图像和问题的答案。例如，FLAVA可以通过多模态编码器融合图像和文本特征，然后通过分类层生成答案。