# MaGnn--ccf(C)
## 阅读总结

MaGnn模型的核心思想总结为以下几个关键部分：

1. 整体架构：Siamese 网络 + 共享编码器

   使用孪生网络结构，两个输入（二进制代码与源代码）共享同一个编码器。编码器将两种不同模态的代码映射到同一高维功能表示空间中,通过计算两个表示向量的余弦相似度来判断是否匹配。

2. 三个核心处理阶段

   （1）预处理与词元化

   对源代码和二进制代码进行词元化，将其拆分为有意义的单元, 对两种代码分别训练词元化模型，适应其不同语法结构。

   （2）语义嵌入

   使用 Word2Vec（SkipGram 模型） 将词元转换为语义向量

   (B2SMatch使用的asm2vec就是基于该方法进行的改进)

   （3）图卷积神经网络编码

   将代码视为图结构数据（如控制流图、函数调用图等），使用图卷积网络对图结构进行编码，聚合节点及其邻居的信息，最终输出一个高维向量，表示代码的功能特征。

3. 训练目标与相似度计算

   使用余弦相似度衡量两个代码表示向量之间的相似性。

   训练目标函数为二元交叉熵损失，鼓励匹配对相似度高，非匹配对相似度低：

## 实验记录

该模型的代码没有开源，为了作为baseline，我尝试在B2SMatch的数据集上进行复现：

1. 数据来源与特征构造

   从 data/507/asm 与 data/507/src 读取文件，按去掉扩展名后的文件名取交集配对。

   将每个文件做简单分词（字母数字下划线 token），用哈希袋向量映射到定长特征（默认维度 in_dim=128），每个文件对应一个"单节点图"的特征向量。



2. 数据划分与加载

   固定随机种子 42，按 80/20 划分训练/测试。

   使用 PairedGraphTensorDataset 与 simple_collate，将一个 batch 内多个"单节点图"拼接，构建对应的 batch 索引。

  

3. 模型结构

   MaGnn 含一个模态共享编码器（此处为 MLP 近似 GCN 的最小占位实现，后续可替换为 PyG 的 GCNConv）；

   两个投影头分别映射到二进制/源码表征空间；最终在单位球面上归一化。

  

4. 训练目标（对比学习）

   使用对称 InfoNCE：对齐同一对 (asm, src) 的表征，拉近正样本，推远同 batch 的其他样本。

5. 实现要点：构建相似度矩阵，标签为对角（配对索引），对行与列各做一次 CrossEntropy 并平均。



6. 训练循环

   前向：编码器+投影头得到 z_bin, z_src

   计算 InfoNCE 损失并反向传播

   记录并打印平均训练损失



7. 测试与检索指标

   在测试集上，分别计算二进制→源码（B>S）与源码→二进制（S>B）的检索相似度矩阵；

   指标：

   - MRR：平均倒数排名，衡量正确匹配排位靠前程度
   - Recall@k：对每个查询，正确匹配是否在前 k；再对所有查询取平均
   - Top@k：正确匹配恰好排第 1 的比例（以 k 列表同样输出，Top@1 常用）

   通过MRR结果和Recall@k结果显示在和B2SMatch的数据相比存在差距，所以可以作为一个baseline作为对照：

![MaGnn](png/MaGnn.jpg)

