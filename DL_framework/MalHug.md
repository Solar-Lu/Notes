
### 一、研究背景  
1. 大模型时代，模型和数据集集中在 Hugging Face 等平台复用。  
2. 复用带来效率，也带来“供应链”风险：有人把恶意代码塞进模型或数据集，使用者一加载就被攻击。  

### 二、两大攻击面  
1. 数据集加载脚本：Hugging Face 默认执行同名 .py 文件，攻击者可嵌入任意代码。  
2. 不安全序列化格式：PyTorch/Keras/TF 大量采用 pickle、HDF5-Lambda、TF-SavedModel 等可被“投毒”的格式，加载即执行。  

### 三、现有工具短板  
Fickling、ModelScan、PickleScan 等只查“有没有危险 API”，无法做语义级分析，容易漏掉混淆或动态拼接的 payload。  

### 四、作者贡献  
1. 首次系统性研究模型仓库投毒：威胁建模 + 15 种格式安全分类 + 根因分析。  
2. 设计并实现 MalHug：  
   • 提取数据集脚本  
   • 反序列化模型文件  
   • 污点分析跟踪数据流  
   • YARA 模式匹配补漏  
3. 与蚂蚁集团联合部署：  
   • 3 个月镜像 Hugging Face，监控 705 K 模型 + 176 K 数据集  
   • 检出 91 个恶意模型 + 9 个恶意数据集脚本  
   • 攻击行为：远程 shell、浏览器凭证窃取、系统侦察  

### 五、三大典型案例  
Case 1 远程控制：PyTorch 权重里藏反向 shell，跨平台连 192.248.1.167:4242。  
Case 2 浏览器大盗：数据集脚本启动即偷 Chrome 密码 + Cookie，经 Discord Webhook 外传。  
Case 3 系统侦察：数据集脚本把 `uname -a` 和 `ps auxww` 结果用 curl 发到 Pipedream。  

### 六、防御与落地  
• 平台：HF 计划默认关闭远程脚本执行；Keras 已修 Lambda 漏洞。  
• 用户：  
  – 用 Safetensors 等安全格式  
  – 容器或无网环境加载可疑模型  
  – 集成 MalHug 做 CI/CD 扫描  
• 社区：持续更新 YARA 规则，对抗混淆和新型攻击。  

一句话收尾：论文首次把“模型即代码”的风险量化并给出工业级检测方案，填补了预训练模型供应链安全的关键空白。